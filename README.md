# FastAPI + Celery + Redis + LLM Example (OpenAI with Hugging Face Fallback)

This application demonstrates processing long-running Large Language Model (LLM) tasks asynchronously using FastAPI, Celery, and Redis. It prioritizes using the OpenAI API but includes a fallback mechanism to use a local Hugging Face model (`distilgpt2`) if the `OPENAI_API_KEY` is not provided in the environment or a `.env` file.

The content of this readme is generated by AI with minor changes. 

## Prerequisites

1.  **Python:** Version 3.12 recommended.
2.  **Redis:** A running Redis server instance.
3.  **OpenAI API Key (Optional):** You need an API key from OpenAI to use their service. If not provided, the app will fall back to the local model.
4.  **Internet Connection (for first run):** The Hugging Face model (`distilgpt2`) and its tokenizer will be downloaded automatically the first time a Celery worker starts without an OpenAI key. This requires an internet connection and some disk space (~500MB).

## Installation

1.  **Clone the repository (or save the files):**
    ```bash
    # If you have a git repo:
    # git clone <your-repo-url>
    # cd <your-repo-directory>

    # Otherwise, just create a directory and save app.py, requirements.txt,
    # and this README.md file in it.
    mkdir celery-llm-fastapi
    cd celery-llm-fastapi
    # <Save files here>
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv ucu_apps_2025_celery_example 
    # On Windows:
    # .\ucu_apps_2025_celery_example\Scripts\activate
    # On macOS/Linux:
    source ucu_apps_2025_celery_example/bin/activate
    ```

    Alternatively
    ```bash
    # On macOS/Linux
    pyenv virtualenv 3.12 ucu_apps_2025_celery_example
    pyenv activate ucu_apps_2025_celery_example
    ```

3.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Install and Start Redis:**

    *   **macOS (using Homebrew):**
        ```bash
        brew install redis
        brew services start redis
        ```
        (Or run `redis-server` directly in a separate terminal)

    *   **Linux (Debian/Ubuntu):**
        ```bash
        sudo apt update
        sudo apt install redis-server
        sudo systemctl start redis-server
        sudo systemctl enable redis-server
        ```

    *   **Linux (Fedora/CentOS):**
        ```bash
        sudo dnf install redis
        sudo systemctl start redis
        sudo systemctl enable redis
        ```

    *Verify Redis is running:*
    ```bash
    redis-cli ping
    ```

    And you should also see redis server output in console.

    ```bash
    18470:C 06 Apr 2025 18:52:54.173 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
    18470:C 06 Apr 2025 18:52:54.173 * Redis version=7.2.7, bits=64, commit=00000000, modified=0, pid=18470, just started
    18470:M 06 Apr 2025 18:52:54.174 * monotonic clock: POSIX clock_gettime
                    _._                                                  
            _.-``__ ''-._                                             
        _.-``    `.  `_.  ''-._           Redis 7.2.7 (00000000/0) 64 bit
    .-`` .-```.  ```\/    _.,_ ''-._                                  
    (    '      ,       .-`  | `,    )     Running in standalone mode
    |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
    |    `-._   `._    /     _.-'    |     PID: 18470
    `-._    `-._  `-./  _.-'    _.-'                                   
    |`-._`-._    `-.__.-'    _.-'_.-'|                                  
    |    `-._`-._        _.-'_.-'    |           https://redis.io       
    `-._    `-._`-.__.-'_.-'    _.-'                                   
    |`-._`-._    `-.__.-'    _.-'_.-'|                                  
    |    `-._`-._        _.-'_.-'    |                                  
    `-._    `-._`-.__.-'_.-'    _.-'                                   
        `-._    `-.__.-'    _.-'                                       
            `-._        _.-'                                           
                `-.__.-'                                               

    18470:M 06 Apr 2025 18:52:54.175 # WARNING: The TCP backlog setting of 511 cannot be enforced because kern.ipc.somaxconn is set to the lower value of 128.
    18470:M 06 Apr 2025 18:52:54.175 * Server initialized
    18470:M 06 Apr 2025 18:52:54.175 * Ready to accept connections tcp
    ```

5.  **Set up OpenAI API Key (Optional):**
    *   **To use OpenAI:** Create a file named `.env` in the project root directory (the same directory as `app.py`). Add your OpenAI API key to this file:
        ```plaintext
        # .env
        OPENAI_API_KEY=your_actual_openai_api_key_here
        ```
        **Important:** Ensure this `.env` file is added to your `.gitignore` if you are using Git.
    *   **To use the Hugging Face fallback:** Simply **do not** create the `.env` file or ensure the `OPENAI_API_KEY` variable is not set in your environment.

## Running the Application

You need to run three components, typically in separate terminal windows/tabs (make sure your virtual environment is activated in each):

1.  **Start the Redis Server:** (Ensure it's running as per installation steps).

2.  **Start the Celery Worker:**
    (From the project root directory where `app.py` is located)
    ```bash
    celery -A app.celery_app worker --loglevel=info
    ```
    *   `-A app.celery_app`: Points Celery to the `celery_app` instance inside `app.py`.
    *   `worker`: Starts the worker process.
    *   `--loglevel=info`: Sets the logging level.
    *   **Note:** If using the Hugging Face fallback, the first time you start the worker, it will download the `distilgpt2` model. This might take a few minutes depending on your internet speed. Subsequent starts will be faster. You'll see log messages indicating this.

3.  **Start the FastAPI Application:**
    (From the project root directory)
    ```bash
    uvicorn app:app --reload --port 8000
    ```
    *   `app:app`: Points Uvicorn to the `app` instance (FastAPI app) inside `app.py`.
    *   `--reload`: Enables auto-reloading for development (optional).
    *   `--port 8000`: Specifies the port (default is 8000).

## Usage

1.  **Check which model is active:**
    Visit `http://localhost:8000/` in your browser or use curl:
    ```bash
    curl http://localhost:8000/
    ```
    The response message will indicate whether OpenAI or the local Hugging Face model is being used.

2.  **Submit a Task:**
    Send a POST request to the `/process` endpoint with your prompt.

    ```bash
    curl -X POST http://localhost:8000/process \
    -H "Content-Type: application/json" \
    -d '{
      "task": "Explain the difference between a list and a tuple in Python."
    }'
    ```

    The response will be immediate:
    ```json
    {
      "task_id": "some-unique-task-id-generated-by-celery",
      "status": "ACCEPTED"
    }
    ```
    Note down the `task_id`.

3.  **Check Task Status:**
    Send a GET request to the `/status/{task_id}` endpoint.

    ```bash
    curl http://localhost:8000/status/some-unique-task-id-generated-by-celery
    ```

    Possible responses:

    *   **While processing:**
        ```json
        {
          "task_id": "some-unique-task-id-generated-by-celery",
          "status": "IN_PROGRESS",
          "result": null
        }
        ```
    *   **After successful completion:**
        ```json
        {
          "task_id": "some-unique-task-id-generated-by-celery",
          "status": "FINISHED",
          "result": "Lists are mutable... Tuples are immutable..." // (Output from OpenAI or Hugging Face)
        }
        ```
        *(Note: The quality and style of the result will differ significantly between OpenAI and `distilgpt2`)*
    *   **If failed:**
        ```json
        {
            "task_id": "some-unique-task-id-generated-by-celery",
            "status": "FAILED",
            "result": {
                "error": "ErrorTypeName", // e.g., "OpenAI API Error", "RuntimeError", etc.
                "details": "Specific error message..."
            }
        }
        ```

## Example Prompts for Longer Processing (10-15 seconds with OpenAI)

These prompts are designed to take longer with OpenAI. Processing time with the local `distilgpt2` model will depend heavily on your CPU performance and might be faster or slower, but likely less coherent for complex tasks.

1.  **Detailed Explanation:**
    ```bash
    curl -X POST http://localhost:8000/process -H "Content-Type: application/json" -d '{ "task": "Explain the concept of quantum entanglement in detail, using analogies suitable for a high school student, covering its history, implications, and potential applications. Ensure the explanation is at least 400 words long." }'
    ```

2.  **Creative Writing with Constraints:**
    ```bash
    curl -X POST http://localhost:8000/process -H "Content-Type: application/json" -d '{ "task": "Write a detailed scene (around 500 words) for a fantasy novel where a young apprentice mage accidentally summons a mischievous elemental spirit made of pure moonlight during their final exam. Describe the setting, the summoning ritual going wrong, the appearance and personality of the spirit, and the immediate chaotic aftermath." }'
    ```

3.  **Code Generation and Explanation:**
    ```bash
    curl -X POST http://localhost:8000/process -H "Content-Type: application/json" -d '{ "task": "Generate a Python script that uses the `requests` library to fetch data from a public JSON API (e.g., JSONPlaceholder posts endpoint), processes the data to count the occurrences of each userId, and then prints the counts in descending order. Include comments in the code explaining each major step and add a brief explanation below the code on how it works." }'
    ```

Keep polling the `/status/{task_id}` endpoint for these tasks to observe the `IN_PROGRESS` state before they transition to `FINISHED` or `FAILED`.